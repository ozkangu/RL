# Training Configuration

# Algorithm settings
algorithm:
  name: "PPO"  # Proximal Policy Optimization
  policy: "MlpPolicy"  # Multi-layer perceptron policy

# PPO Hyperparameters (simplified for MVP)
ppo:
  learning_rate: 0.0003
  n_steps: 2048  # Number of steps to run per environment per update
  batch_size: 64  # Minibatch size
  n_epochs: 10  # Number of epochs when optimizing
  gamma: 0.99  # Discount factor
  gae_lambda: 0.95  # Factor for trade-off of bias vs variance
  clip_range: 0.2  # Clipping parameter
  ent_coef: 0.01  # Entropy coefficient (encourages exploration)
  vf_coef: 0.5  # Value function coefficient
  max_grad_norm: 0.5  # Max gradient norm

# Training settings
training:
  total_timesteps: 200000  # Total timesteps to train (MVP: 200k)
  n_envs: 1  # Number of parallel environments (MVP: 1, using DummyVecEnv)
  seed: 42  # Random seed for reproducibility
  use_lr_schedule: true  # Use linear learning rate schedule (decays from initial to 0)
  early_stop_patience: 5  # Stop if no improvement for N evaluations
  early_stop_min_evals: 10  # Minimum evaluations before early stopping kicks in

# Checkpointing
checkpoint:
  save_freq: 10000  # Save model every N steps
  save_path: "ckpts/"  # Directory to save checkpoints
  keep_best: true  # Keep best model based on mean reward

# Logging
logging:
  log_interval: 10  # Log every N episodes
  tensorboard_log: "artifacts/tensorboard/"  # TensorBoard log directory
  verbose: 1  # Verbosity level (0: no output, 1: info, 2: debug)

# Policy Architecture (optional)
# Network architecture for actor (pi) and critic (vf)
policy_kwargs:
  net_arch:
    - pi: [64, 64]  # Actor network: 2 hidden layers with 64 units each
      vf: [64, 64]  # Critic network: 2 hidden layers with 64 units each

# Reproducibility
reproducibility:
  deterministic_pytorch: true
  deterministic_numpy: true
